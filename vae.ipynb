{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "input_size = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dropout': 0.8,\n",
    "    'e_layers': [128],\n",
    "    'd_layers': [128],\n",
    "    'd_std_cold_start': 0.3,\n",
    "    'cold_start_ends': 20,\n",
    "    'z_dim': 100,\n",
    "    'activation': 'sigmoid',\n",
    "    'batch_size': 100,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.0002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_net(x, layers, output_dim):\n",
    "    for layer in layers:\n",
    "        x = tf.layers.dense(x,\n",
    "                            layer,\n",
    "                            activation={'tanh': tf.nn.tanh, 'sigmoid': tf.nn.sigmoid}[params['activation']])\n",
    "        x = tf.nn.dropout(x, params['dropout'])\n",
    "    mu = tf.layers.dense(x, output_dim)\n",
    "    var = 1e-5 + tf.exp(tf.layers.dense(x, output_dim))\n",
    "    return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our input placeholders\n",
    "images = tf.placeholder(tf.float32, (None, input_size), name='image')\n",
    "\n",
    "# Build the model\n",
    "e_mu, e_var = stochastic_net(images, params['e_layers'], params['z_dim'])\n",
    "eps = tf.random_normal([tf.shape(images)[0], params['z_dim']], mean=0.0, stddev=1.0)\n",
    "z = e_mu + tf.sqrt(e_var) * eps\n",
    "d_mu, d_var = stochastic_net(z, params['d_layers'], input_size)\n",
    "d_mu = tf.nn.sigmoid(d_mu)\n",
    "\n",
    "num_batches = mnist.train.num_examples//params['batch_size']\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "d_std = tf.cond(global_step < params['cold_start_ends'] * num_batches,\n",
    "                lambda: params['d_std_cold_start'],\n",
    "                lambda: tf.sqrt(d_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "loss_encode = - 0.5 * tf.reduce_sum(1 + tf.log(e_var) - e_mu ** 2 - e_var, axis=1)\n",
    "loss_decode = - tf.reduce_sum(tf.contrib.distributions.Normal(d_mu, d_std).log_prob(images), axis=1)\n",
    "loss = tf.reduce_mean(loss_encode + loss_decode, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "train_op = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch 1/50...', 'Loss: 67.0471')\n",
      "('Epoch 2/50...', 'Loss: 65.0508')\n",
      "('Epoch 3/50...', 'Loss: 61.8645')\n",
      "('Epoch 4/50...', 'Loss: 43.5620')\n",
      "('Epoch 5/50...', 'Loss: 24.5086')\n",
      "('Epoch 6/50...', 'Loss: 0.0049')\n",
      "('Epoch 7/50...', 'Loss: -13.7173')\n",
      "('Epoch 8/50...', 'Loss: -17.1002')\n",
      "('Epoch 9/50...', 'Loss: -0.8542')\n",
      "('Epoch 10/50...', 'Loss: -23.8613')\n",
      "('Epoch 11/50...', 'Loss: -20.4175')\n",
      "('Epoch 12/50...', 'Loss: -26.0050')\n",
      "('Epoch 13/50...', 'Loss: -25.8658')\n",
      "('Epoch 14/50...', 'Loss: -29.8176')\n",
      "('Epoch 15/50...', 'Loss: -30.6528')\n",
      "('Epoch 16/50...', 'Loss: -34.4893')\n",
      "('Epoch 17/50...', 'Loss: -43.9566')\n",
      "('Epoch 18/50...', 'Loss: -46.2164')\n",
      "('Epoch 19/50...', 'Loss: -38.4015')\n",
      "('Epoch 20/50...', 'Loss: 755.1227')\n",
      "('Epoch 21/50...', 'Loss: -1442.5038')\n",
      "('Epoch 22/50...', 'Loss: -1548.2920')\n",
      "('Epoch 23/50...', 'Loss: -1566.0380')\n",
      "('Epoch 24/50...', 'Loss: -1642.9843')\n",
      "('Epoch 25/50...', 'Loss: -1852.0916')\n",
      "('Epoch 26/50...', 'Loss: -1838.0902')\n",
      "('Epoch 27/50...', 'Loss: -1767.4000')\n",
      "('Epoch 28/50...', 'Loss: -1803.9465')\n",
      "('Epoch 29/50...', 'Loss: -1932.0394')\n",
      "('Epoch 30/50...', 'Loss: -1841.3209')\n",
      "('Epoch 31/50...', 'Loss: -1894.6787')\n",
      "('Epoch 32/50...', 'Loss: -1979.6908')\n",
      "('Epoch 33/50...', 'Loss: -1969.7695')\n",
      "('Epoch 34/50...', 'Loss: -1980.4587')\n",
      "('Epoch 35/50...', 'Loss: -2007.8459')\n",
      "('Epoch 36/50...', 'Loss: -1855.9137')\n",
      "('Epoch 37/50...', 'Loss: -1939.4062')\n",
      "('Epoch 38/50...', 'Loss: -2039.9845')\n",
      "('Epoch 39/50...', 'Loss: -2032.3810')\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "losses = []\n",
    "# Only save generator variables\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(params['epochs']):\n",
    "        for ii in range(num_batches):\n",
    "            batch = mnist.train.next_batch(params['batch_size'])\n",
    "            \n",
    "            # Get images\n",
    "            batch_images = batch[0].reshape((params['batch_size'], input_size))\n",
    "            \n",
    "            # Sample random noise for G\n",
    "            batch_z = np.random.uniform(-1, 1, size=(params['batch_size'], params['z_dim']))\n",
    "            \n",
    "            # Run optimizer\n",
    "            sess.run(train_op, feed_dict={images: batch_images})\n",
    "        \n",
    "        # At the end of each epoch, get the loss and print it out\n",
    "        train_loss = sess.run(loss, {images: batch_images})\n",
    "            \n",
    "        print(\"Epoch {}/{}...\".format(e+1, params['epochs']),\n",
    "              \"Loss: {:.4f}\".format(train_loss))\n",
    "        # Save losses to view after training\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        # Sample an image as we're training for viewing afterwards\n",
    "        sample_z = np.random.randn(16, params['z_dim'])\n",
    "        gen_samples = sess.run(\n",
    "                       d_mu,\n",
    "                       feed_dict={z: sample_z})\n",
    "        samples.append(gen_samples)\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "        if np.isnan(train_loss):\n",
    "            print 'loss is NaN!'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "    \n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows, cols = min(len(samples), 20), 6\n",
    "fig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n",
    "\n",
    "for sample, ax_row in zip(samples[::int(len(samples)/rows)], axes):\n",
    "    for img, ax in zip(sample[::int(len(sample)/cols)], ax_row):\n",
    "        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
